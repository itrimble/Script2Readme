{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fnil\fcharset0 HelveticaNeue-Bold;\f2\fnil\fcharset0 .SFNS-Regular_wdth_opsz110000_GRAD_wght1F40000;
\f3\fnil\fcharset0 .SFNS-Regular_wdth_opsz168000_GRAD_wght1F40000;\f4\fnil\fcharset0 .AppleSystemUIFontMonospaced-Regular;}
{\colortbl;\red255\green255\blue255;\red16\green16\blue16;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c7843\c7843\c7451;\cssrgb\c100000\c100000\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid1\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid2\'00;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww17800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf2 IT
\f2\b0\fs21 \AppleTypeServices\AppleTypeServicesF65539 \
Ian Trimble\
\pard\pardeftab720\qc\partightenfactor0

\f0\fs18 \AppleTypeServices \cf2 Pro plan\
\pard\pardeftab720\qc\partightenfactor0

\f2\fs21 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://claude.ai/project/019681f3-f1a8-75a0-9c4f-6a625e735306"}}{\fldrslt 
\f0 \AppleTypeServices \cf2 Script2Readme\'a0/\
}}\pard\pardeftab720\qc\partightenfactor0

\f0 \AppleTypeServices \cf2 Untitled\
\
\pard\pardeftab720\qc\partightenfactor0

\f2 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://claude.ai/new"}}{\fldrslt 
\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
}}\pard\pardeftab720\qc\partightenfactor0

\f3\fs45 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf2 What can I help you with today?
\f0\fs24 \AppleTypeServices \
\pard\pardeftab720\partightenfactor0

\fs32 \cf2 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
Analyze this script for errors and potential problems. Suggest fixes\
\pard\pardeftab720\qc\partightenfactor0

\fs32 \cf2 \
\
\
\
\pard\pardeftab720\partightenfactor0

\fs18 \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\fs28 \cf2 \
3.7 Sonnet\
\pard\pardeftab720\qc\partightenfactor0

\fs21 \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\f2\fs32 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0

\f0\fs24 \AppleTypeServices \cf2 \
\pard\pardeftab720\partightenfactor0
\cf2 script2readme.sh
\fs20 1,916 lines
\fs32 \
\pard\pardeftab720\partightenfactor0

\f2\fs22 \AppleTypeServices\AppleTypeServicesF65539 \cf2 SH\
\pard\pardeftab720\qc\partightenfactor0

\f0\fs32 \AppleTypeServices \cf2 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
\
\pard\pardeftab720\qc\partightenfactor0

\f2\fs21 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\
\pard\pardeftab720\qc\partightenfactor0

\f0 \AppleTypeServices \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\fs28 \cf2 \
\pard\pardeftab720\partightenfactor0

\f2\fs27 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\f0\fs32 \AppleTypeServices \cf2 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
\
\pard\pardeftab720\partightenfactor0

\fs21 \cf2 \
\pard\pardeftab720\sa120\partightenfactor0

\f2 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://claude.ai/project/019681f3-f1a8-75a0-9c4f-6a625e735306"}}{\fldrslt 
\f0\fs24 \AppleTypeServices \cf2 \
\
}}\pard\pardeftab720\partightenfactor0

\f0\fs24 \AppleTypeServices \cf2 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1
\f2\fs22 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 		\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \uc0\u8232 
\f0\fs32 \AppleTypeServices \uc0\u8232 \u8232 \u8232 
\fs24 \uc0\u8232 \u8232 
\f2\fs22 \AppleTypeServices\AppleTypeServicesF65539 \uc0\u8232 
\f0\fs32 \AppleTypeServices \uc0\u8232 \u8232 \u8232 
\fs24 \uc0\u8232 \u8232 
\f2\fs22 \AppleTypeServices\AppleTypeServicesF65539 \uc0\u8232 
\f0\fs32 \AppleTypeServices \uc0\u8232 \u8232 \u8232 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sa120\partightenfactor0

\f2\fs21 \AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\f0\fs24 \AppleTypeServices \cf2 \
\pard\pardeftab720\qc\partightenfactor0

\fs18 \cf2 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
\pard\pardeftab720\partightenfactor0
\AppleTypeServices\AppleTypeServicesF65539 \cf2 \
\pard\pardeftab720\partightenfactor0

\f2\fs27 \AppleTypeServices\AppleTypeServicesF65539 \cf2 script2readme.sh
\f0\fs32 \AppleTypeServices \
\pard\pardeftab720\qc\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\fs18 \cf2 76.67 KB\'a0\'951,916 lines\
\'95\
Formatting may be inconsistent from source\
\pard\pardeftab720\partightenfactor0

\f4 \cf2 #!/bin/zsh\
#\
# script2readme.sh - Generate README documentation from scripts using Ollama models\
# Author: Ian Trimble\
# Created: April 28, 2025\
# Version: 1.5.0\
#\
\
# Enable debug mode only when explicitly requested\
if [[ "$1" == "--debug" ]]; then\
  set -x\
  shift\
fi\
\
# =================== CONFIGURATION ===================\
# App information\
APP_NAME="Script to README Generator"\
APP_VERSION="1.5.0"\
APP_AUTHOR="Ian Trimble"\
\
# Directory structure\
BENCHMARK_DIR="$\{HOME\}/ollama_benchmarks"\
PROJECT_DIR="$(pwd)"\
SESSION_ID=$(date +%Y%m%d_%H%M%S)_$(openssl rand -hex 4)\
BENCHMARK_LOG="$\{BENCHMARK_DIR\}/benchmark_log.csv"\
METRICS_LOG="$\{BENCHMARK_DIR\}/metrics_$\{SESSION_ID\}.json"\
CHANGELOG="$\{PROJECT_DIR\}/CHANGELOG.md"\
README="$(pwd)/README.md"\
OLLAMA_API="http://localhost:11434/api/chat"\
\
# Colors and formatting (from enhanced_script.sh)\
RED='\\033[0;31m'\
GREEN='\\033[0;32m'\
YELLOW='\\033[0;33m'\
BLUE='\\033[0;34m'\
MAGENTA='\\033[0;35m'\
CYAN='\\033[0;36m'\
WHITE='\\033[1;37m'\
GRAY='\\033[0;37m'\
BOLD='\\033[1m'\
RESET='\\033[0m'\
\
# Check if terminal supports colors\
if [ ! -t 1 ]; then\
  # Reset all color variables to empty strings if not in a terminal\
  RED='' GREEN='' YELLOW='' BLUE='' MAGENTA='' CYAN='' WHITE='' GRAY=''\
  BOLD='' RESET=''\
fi\
\
# Did you know tips\
declare -a TIPS\
TIPS=(\
  "You can select different models for different scripts to compare documentation quality."\
  "Larger models (13B+) generally produce more detailed documentation but take longer."\
  "Your benchmarks are saved to $\{BENCHMARK_DIR\} for performance analysis."\
  "The script automatically detects base64-encoded files and decodes them."\
  "Script metrics like function count and line count help estimate processing time."\
  "Good documentation can reduce project onboarding time by up to 60%."\
  "The script supports multiple script types: shell, python, ruby, javascript, and more."\
  "Historical performance data improves time estimates with each run."\
  "The generated README includes detailed metadata about your script."\
  "Future versions will support batch processing and interactive editing."\
)\
\
# Default model will be set dynamically based on available models\
DEFAULT_MODEL=""\
\
# Model complexity factors will be assigned dynamically\
declare -A MODEL_COMPLEXITY\
\
# Model complexity estimation based on parameter size\
# These are base factors - actual values will be calculated dynamically\
declare -A MODEL_SIZE_COMPLEXITY\
MODEL_SIZE_COMPLEXITY["1-3B"]=1.0   # Small models (1-3B parameters)\
MODEL_SIZE_COMPLEXITY["4-7B"]=2.5   # Medium models (4-7B parameters)\
MODEL_SIZE_COMPLEXITY["8-13B"]=4.0  # Large models (8-13B parameters)\
MODEL_SIZE_COMPLEXITY["14B+"]=6.0   # Extra large models (14B+ parameters)\
\
# Default for unknown models\
MODEL_COMPLEXITY["default"]=2.5\
\
# Create benchmark directory if it doesn't exist\
mkdir -p "$\{BENCHMARK_DIR\}"\
\
# Initialize benchmark file if it doesn't exist\
if [ ! -f "$\{BENCHMARK_LOG\}" ]; then\
  echo "timestamp,session_id,script_name,script_size_bytes,script_lines,script_chars,model,operation,duration,tokens,cpu_usage,memory_usage" > "$\{BENCHMARK_LOG\}"\
fi\
\
# Create project directory if not exists (should be present, but just in case)\
mkdir -p "$\{PROJECT_DIR\}"\
\
# Initialize changelog if it doesn't exist\
if [ ! -f "$\{CHANGELOG\}" ]; then\
  \{\
    echo "# Script to README Generator Changelog"\
    echo ""\
    echo "## Version 1.4.3 - $(date '+%Y-%m-%d')"\
    echo "- Enhanced prompt engineering for more comprehensive READMEs"\
    echo "- Added colorized output for better terminal experience"\
    echo "- Improved base64 file detection and decoding"\
    echo "- Added \\"Did you know\\" tips system for user guidance"\
    echo "- Enhanced README output with detailed script metadata table"\
    echo "- Added model performance information section"\
    echo "- Improved JSON request handling for special characters"\
    echo "- Fixed API error handling and response parsing"\
  \} > "$\{CHANGELOG\}"\
  echo -e "$\{GREEN\}Created new changelog at $\{CHANGELOG\}$\{RESET\}"\
else\
  # Check if version entry exists, add if not\
  if ! grep -q "## Version $\{APP_VERSION\}" "$\{CHANGELOG\}"; then\
    sed -i '' "1a\\\\\
\\\\\
## Version $\{APP_VERSION\} - $(date '+%Y-%m-%d')\\\\\
- Significantly enhanced system prompt for more comprehensive and better structured READMEs\\\\\
- Added specific section requirements with mandatory templates for consistent documentation\\\\\
- Implemented model-specific prompt optimizations for deepseek-coder and codellama models\\\\\
- Improved time estimation algorithm with model-specific timing adjustments\\\\\
- Extended minimum processing times to ensure high-quality output from all models\\\\\
- Enhanced markdown formatting instructions with specific guidelines for code blocks and lists\\\\\
- Added word count requirements for more thorough documentation\\\\\
- Improved error handling during API requests with better feedback\
" "$\{CHANGELOG\}"\
    echo -e "$\{GREEN\}Updated changelog at $\{CHANGELOG\}$\{RESET\}"\
  fi\
fi\
\
# Initialize metrics JSON log\
echo "\{\\"session_id\\": \\"$\{SESSION_ID\}\\", \\"timestamp\\": \\"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\\", \\"app_version\\": \\"$\{APP_VERSION\}\\", \\"metrics\\": []\}" > "$\{METRICS_LOG\}"\
\
# =================== HELPER FUNCTIONS ===================\
# Function to display progress\
display_progress() \{\
  local progress=$1\
  local duration=$2\
  local width=50\
  local filled=$((width * progress / 100))\
  local empty=$((width - filled))\
  \
  # Create the progress bar\
  printf "\\r["\
  printf "%$\{filled\}s" '' | tr ' ' '='\
  printf ">"\
  printf "%$\{empty\}s" '' | tr ' ' ' '\
  printf "] %3d%% (%s)" $progress "$duration"\
\}\
\
# Function to estimate completion time\
estimate_completion_time() \{\
  local script_size=$1\
  local model=$2\
  local line_count=$3\
  \
  # Get model complexity factor\
  local complexity=$\{MODEL_COMPLEXITY[$model]\}\
  if [ -z "$complexity" ]; then\
    complexity=$\{MODEL_COMPLEXITY["default"]\}\
  fi\
  \
  # Base time in seconds per KB for a standard model\
  local base_time=2\
  \
  # Check if we have historical data for this model\
  local historical_data=""\
  local historical_factor=1.0\
  if [ -f "$\{BENCHMARK_DIR\}/model_performance.csv" ]; then\
    # Format: model,avg_time_per_kb,samples\
    historical_data=$(grep "^$\{model\}," "$\{BENCHMARK_DIR\}/model_performance.csv" 2>/dev/null)\
    if [ -n "$historical_data" ]; then\
      local avg_time=$(echo "$historical_data" | cut -d, -f2)\
      \
      # Validate avg_time is a valid number\
      if [[ "$avg_time" =~ ^[0-9]+(\\.[0-9]+)?$ ]]; then\
        # Calculate historical factor but ensure division by zero is avoided\
        if (( $(echo "$base_time > 0" | bc -l 2>/dev/null) )) && (( $(echo "$complexity > 0" | bc -l 2>/dev/null) )); then\
          historical_factor=$(echo "scale=2; $avg_time / ($base_time * $complexity)" | bc 2>/dev/null)\
          \
          # Validate result is a reasonable number\
          if [ -z "$historical_factor" ] || [[ ! "$historical_factor" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || \
             (( $(echo "$historical_factor <= 0" | bc -l 2>/dev/null) )) || \
             (( $(echo "$historical_factor > 100" | bc -l 2>/dev/null) )); then\
            echo "Warning: Invalid historical factor calculated ($historical_factor). Using default."\
            historical_factor=1.0\
          else\
            echo "Using historical performance data for $model (factor: $historical_factor)" >> "$\{BENCHMARK_DIR\}/estimation_log.txt"\
          fi\
        else\
          echo "Warning: Invalid base_time or complexity for historical factor calculation. Using default."\
          historical_factor=1.0\
        fi\
      else\
        echo "Warning: Invalid average time in historical data ($avg_time). Using default."\
        historical_factor=1.0\
      fi\
    else\
      echo "No historical data found for $model. Using default factor." >> "$\{BENCHMARK_DIR\}/estimation_log.txt"\
    fi\
  else\
    echo "No model performance database found. Using default factor." >> "$\{BENCHMARK_DIR\}/estimation_log.txt"\
  fi\
  \
  # Adjust based on script size and line count\
  # Larger scripts may take disproportionately longer\
  # Scripts with more lines may be more complex\
  local size_factor\
  local line_factor\
  \
  # Calculate size factor (with error handling)\
  size_factor=$(printf "%.2f" $(echo "scale=2; ($script_size / 1024) ^ 0.6" | bc 2>/dev/null))\
  \
  # Make sure size_factor is a valid number and at least 1.0\
  if [ -z "$size_factor" ] || ! [[ "$size_factor" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || (( $(echo "$size_factor < 1.0" | bc -l 2>/dev/null) )); then\
    echo "Resetting invalid size factor ($size_factor) to 1.0"\
    size_factor=1.0\
  fi\
  \
  # Calculate line complexity factor (with error handling)\
  if [ $line_count -gt 0 ]; then\
    # More lines per KB indicates more complex code\
    local lines_per_kb=$(echo "scale=2; $line_count / ($script_size / 1024)" | bc 2>/dev/null)\
    if [ -n "$lines_per_kb" ] && (( $(echo "$lines_per_kb > 0" | bc 2>/dev/null) )); then\
      # Normalize to a factor between 0.8 and 1.5\
      line_factor=$(echo "scale=2; 0.8 + (($lines_per_kb / 40) * 0.7)" | bc 2>/dev/null)\
      # Cap at reasonable bounds\
      if (( $(echo "$line_factor > 1.5" | bc 2>/dev/null) )); then\
        line_factor=1.5\
      elif (( $(echo "$line_factor < 0.8" | bc 2>/dev/null) )); then\
        line_factor=0.8\
      fi\
    else\
      line_factor=1.0\
    fi\
  else\
    line_factor=1.0\
  fi\
  \
  # Calculate estimated seconds with error handling\
  local estimate\
  \
  # Make sure all factors are valid for calculation\
  if [ -z "$base_time" ] || [ -z "$size_factor" ] || [ -z "$complexity" ] || [ -z "$line_factor" ] || [ -z "$historical_factor" ]; then\
    echo "Warning: Missing factors for time calculation. Using default estimate."\
    estimate=30\
  else\
    # Log factors for debugging\
    echo "Calculation factors: base_time=$base_time, size_factor=$size_factor, complexity=$complexity, line_factor=$line_factor, historical_factor=$historical_factor"\
    \
    # Force factors to reasonable values if needed\
    [ -z "$base_time" ] && base_time=2\
    [ "$size_factor" = "0" ] || [ -z "$size_factor" ] && size_factor=1.0\
    [ "$complexity" = "0" ] || [ -z "$complexity" ] && complexity=2.5\
    [ "$line_factor" = "0" ] || [ -z "$line_factor" ] && line_factor=1.0\
    [ "$historical_factor" = "0" ] || [ -z "$historical_factor" ] && historical_factor=1.0\
    \
    # Calculate the estimate with non-zero factors\
    estimate=$(printf "%.0f" $(echo "scale=0; $base_time * $size_factor * $complexity * $line_factor * $historical_factor" | bc 2>/dev/null))\
    \
    # Validate the estimate is a number and at least 10\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]]; then\
      echo "Warning: Estimate calculation failed. Using default estimate."\
      estimate=30\
    elif [ "$estimate" -eq 0 ]; then\
      echo "Warning: Estimate calculated as 0. Using default estimate."\
      estimate=30\
    fi\
  fi\
  \
  # Log estimation factors for analysis\
  echo "$(date '+%Y-%m-%d %H:%M:%S') - Model: $model, Size: $\{script_size\}B, Lines: $line_count, Complexity: $complexity, SizeFactor: $size_factor, LineFactor: $line_factor, HistoricalFactor: $historical_factor, Estimate: $\{estimate\}s" >> "$\{BENCHMARK_DIR\}/estimation_log.txt"\
  \
  # Ensure minimum reasonable time\
  if [ $estimate -lt 10 ] || [ -z "$estimate" ]; then\
    estimate=10\
  fi\
  \
  # Apply model-specific time adjustments based on benchmarks and known patterns\
  # Set minimum times for specific models based on previous benchmark data\
  if [[ "$model" == "deepseek-coder:latest" ]]; then\
    # From benchmarks: deepseek-coder:latest can take a while for large files\
    local min_time=180\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]] || [ "$estimate" -lt $min_time ]; then\
      echo "Adjusting estimate for $\{model\} based on historical data (from $\{estimate\}s to $\{min_time\}s)"\
      echo "Note: deepseek-coder:latest needs extra time for enhanced documentation quality"\
      estimate=$min_time\
    fi\
  elif [[ "$model" == "qwen2.5-coder:7b" ]]; then\
    # From benchmarks: qwen2.5-coder:7b consistently takes ~55s for large files\
    local min_time=80\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]] || [ "$estimate" -lt $min_time ]; then\
      echo "Adjusting estimate for $\{model\} based on historical data (from $\{estimate\}s to $\{min_time\}s)"\
      estimate=$min_time\
    fi\
  elif [[ "$model" == *"codellama"* ]]; then\
    # Codellama models need extra time for enhanced documentation quality\
    local min_time=150\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]] || [ "$estimate" -lt $min_time ]; then\
      echo "Adjusting estimate for $\{model\} based on enhanced documentation requirements (from $\{estimate\}s to $\{min_time\}s)"\
      echo "Note: Codellama models need extra time to generate high-quality documentation"\
      estimate=$min_time\
    fi\
  elif [[ "$model" == *"llama:70b"* || "$model" == *"deepseek-coder"* || "$model" == *"mixtral:8x7b"* \
       || "$model" == *"grok"* || "$model" == *"phi"* ]]; then\
    # For other large models, use generic large model minimum - be generous!\
    local min_time=120\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]] || [ "$estimate" -lt $min_time ]; then\
      if ! [[ "$estimate" =~ ^[0-9]+$ ]]; then\
        echo "Warning: Invalid estimate. Using default for $\{model\}."\
        estimate=$min_time\
      else\
        echo "Adjusting estimate upwards for large model $\{model\} (from $\{estimate\}s to $\{min_time\}s)"\
        estimate=$min_time\
      fi\
    fi\
  else\
    # For any other model, ensure a minimum of 90 seconds to account for enhanced documentation\
    local min_time=90\
    if [ -z "$estimate" ] || ! [[ "$estimate" =~ ^[0-9]+$ ]] || [ "$estimate" -lt $min_time ]; then\
      echo "Ensuring minimum estimate of $\{min_time\}s for $\{model\} with enhanced documentation requirements"\
      estimate=$min_time\
    fi\
  fi\
  \
  # Scale estimate based on file size for large files (>10KB)\
  if [ "$script_size" -gt 10240 ] && [ "$estimate" -gt 0 ]; then\
    local size_scale=$(echo "scale=2; ($script_size / 10240) ^ 0.6" | bc 2>/dev/null)\
    if [[ "$size_scale" =~ ^[0-9]+(\\.[0-9]+)?$ ]] && (( $(echo "$size_scale > 1.0" | bc -l 2>/dev/null) )); then\
      local original_estimate=$estimate\
      estimate=$(printf "%.0f" $(echo "scale=0; $estimate * $size_scale" | bc 2>/dev/null))\
      echo "Scaling estimate by $\{size_scale\}x for large file of $\{script_size\} bytes ($\{original_estimate\}s \uc0\u8594  $\{estimate\}s)"\
    fi\
  fi\
  \
  echo $estimate\
\}\
\
# Function to format time in human-readable format\
format_time() \{\
  local seconds=$1\
  \
  # Ensure seconds is an integer\
  if [ -z "$seconds" ] || ! [[ "$seconds" =~ ^[0-9]+$ ]]; then\
    echo "Warning: Invalid time value to format: '$seconds'. Using default." >&2\
    seconds=0\
  fi\
  \
  local minutes=$((seconds / 60))\
  local remaining_seconds=$((seconds % 60))\
  \
  if [ $minutes -gt 0 ]; then\
    echo "$\{minutes\}m $\{remaining_seconds\}s"\
  else\
    echo "$\{seconds\}s"\
  fi\
\}\
\
# Function to show a random tip\
show_tip() \{\
  local tip_index=$((RANDOM % $\{#TIPS[@]\}))\
  local tip="$\{TIPS[$tip_index]\}"\
  echo -e "\\n$\{YELLOW\}\uc0\u55357 \u56481  $\{BOLD\}Did you know?$\{RESET\} $\{tip\}$\{RESET\}\\n"\
\}\
\
# Function to display usage information\
show_usage() \{\
  echo -e "$\{CYAN\}$\{BOLD\}$\{APP_NAME\}$\{RESET\} $\{WHITE\}(v$\{APP_VERSION\})$\{RESET\}"\
  echo -e "$\{GRAY\}Generates README documentation from script files using Ollama models$\{RESET\}"\
  echo ""\
  echo -e "$\{YELLOW\}$\{BOLD\}Usage:$\{RESET\} ./script2readme.sh $\{GREEN\}[OPTIONS]$\{RESET\} $\{MAGENTA\}<input_file>$\{RESET\} $\{BLUE\}[model]$\{RESET\}"\
  echo ""\
  echo -e "$\{YELLOW\}$\{BOLD\}Options:$\{RESET\}"\
  echo -e "  $\{GREEN\}--debug$\{RESET\}                Enable debug mode"\
  echo -e "  $\{GREEN\}--help$\{RESET\}                 Show this help message"\
  echo -e "  $\{GREEN\}--list-models$\{RESET\}          List available Ollama models"\
  echo -e "  $\{GREEN\}--version$\{RESET\}              Show version information"\
  echo -e "  $\{GREEN\}--no-estimate$\{RESET\}          Skip time estimation"\
  echo ""\
  echo -e "$\{YELLOW\}$\{BOLD\}Arguments:$\{RESET\}"\
  echo -e "  $\{MAGENTA\}<input_file>$\{RESET\}           Path to script file to document"\
  echo -e "  $\{BLUE\}[model]$\{RESET\}                Optional Ollama model name (default: $\{WHITE\}$\{DEFAULT_MODEL\}$\{RESET\})"\
  echo ""\
  echo -e "$\{YELLOW\}$\{BOLD\}Examples:$\{RESET\}"\
  echo -e "  $\{GRAY\}./script2readme.sh my_script.sh$\{RESET\}"\
  echo -e "  $\{GRAY\}./script2readme.sh my_script.sh codellama:7b$\{RESET\}"\
  echo ""\
  echo -e "$\{YELLOW\}$\{BOLD\}Output:$\{RESET\}"\
  echo -e "  - Creates a new README file (format: README_script_model.md)"\
  echo -e "  - Logs performance metrics and benchmarks to $\{BENCHMARK_DIR\}"\
  echo ""\
  show_tip\
  exit 0\
\}\
\
# Function to display version information\
show_version() \{\
  echo -e "$\{CYAN\}$\{BOLD\}$\{APP_NAME\}$\{RESET\} $\{WHITE\}v$\{APP_VERSION\}$\{RESET\}"\
  echo -e "$\{GRAY\}Author: $\{WHITE\}$\{APP_AUTHOR\}$\{RESET\}"\
  echo -e "$\{GRAY\}Created: April 28, 2025$\{RESET\}"\
  echo -e "$\{GRAY\}License: MIT$\{RESET\}"\
  show_tip\
  exit 0\
\}\
\
# Function to get system information\
get_system_info() \{\
  local cpu_info=$(sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "Unknown")\
  local memory_info=$(sysctl -n hw.memsize 2>/dev/null | awk '\{print int($1/1024/1024/1024) " GB"\}' || echo "Unknown")\
  local os_info=$(sw_vers -productVersion 2>/dev/null || echo "Unknown")\
  local ollama_version=$(ollama --version 2>/dev/null || echo "Unknown")\
  \
  # Add system info to metrics log\
  jq --arg cpu "$\{cpu_info\}" \\\
     --arg mem "$\{memory_info\}" \\\
     --arg os "$\{os_info\}" \\\
     --arg ollama "$\{ollama_version\}" \\\
     '.system_info = \{"cpu": $cpu, "memory": $mem, "os": $os, "ollama_version": $ollama\}' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with system information"\
  fi\
  \
  echo -e "$\{BLUE\}System Info:$\{RESET\} CPU: $\{WHITE\}$\{cpu_info\}$\{RESET\}, Memory: $\{WHITE\}$\{memory_info\}$\{RESET\}, OS: $\{WHITE\}$\{os_info\}$\{RESET\}, Ollama: $\{WHITE\}$\{ollama_version\}$\{RESET\}"\
\}\
\
# Function to get current resource usage\
get_resource_usage() \{\
  local cpu_usage=$(ps -o %cpu= -p $$ | awk '\{print $1\}')\
  local memory_usage=$(ps -o rss= -p $$ | awk '\{print int($1/1024) " MB"\}')\
  \
  echo "$\{cpu_usage\},$\{memory_usage\}"\
\}\
\
# Function to log benchmark data\
log_benchmark() \{\
  local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")\
  local script_name=$1\
  local script_size=$2\
  local script_lines=$3\
  local script_chars=$4\
  local model=$5\
  local operation=$6\
  local duration=$7\
  local token_count=$8\
  \
  # Get resource usage\
  local resource_usage=$(get_resource_usage)\
  local cpu_usage=$(echo $\{resource_usage\} | cut -d, -f1)\
  local memory_usage=$(echo $\{resource_usage\} | cut -d, -f2)\
  \
  # Log to CSV for detailed data\
  echo "$\{timestamp\},$\{SESSION_ID\},$\{script_name\},$\{script_size\},$\{script_lines\},$\{script_chars\},$\{model\},$\{operation\},$\{duration\},$\{token_count\},$\{cpu_usage\},$\{memory_usage\}" >> "$\{BENCHMARK_LOG\}"\
  \
  # Add metric to JSON log\
  jq --arg timestamp "$\{timestamp\}" \\\
     --arg script "$\{script_name\}" \\\
     --arg size "$\{script_size\}" \\\
     --arg lines "$\{script_lines\}" \\\
     --arg chars "$\{script_chars\}" \\\
     --arg model "$\{model\}" \\\
     --arg op "$\{operation\}" \\\
     --arg dur "$\{duration\}" \\\
     --arg tokens "$\{token_count\}" \\\
     --arg cpu "$\{cpu_usage\}" \\\
     --arg mem "$\{memory_usage\}" \\\
     '.metrics += [\{"timestamp": $timestamp, "script": $script, "size_bytes": $size, "line_count": $lines, "char_count": $chars, "model": $model, "operation": $op, "duration": $dur, "token_count": $tokens, "cpu_usage": $cpu, "memory_usage": $mem\}]' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
     \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with benchmark data"\
  fi\
  \
  # Return for chaining\
  echo "$\{operation\}:$\{duration\}"\
\}\
\
# Function to generate a benchmark summary\
generate_benchmark_summary() \{\
  local model=$1\
  local script_name=$2\
  local total_time=$3\
  local api_time=$4\
  local parse_time=$5\
  local script_size=$6\
  local script_lines=$7\
  local script_chars=$8\
  local token_count=$9\
  local estimated_time=$\{10\}\
  \
  echo -e "$\{BLUE\}\uc0\u9556 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9559 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{CYAN\}$\{BOLD\}README GENERATION COMPLETE                         $\{RESET\}$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9568 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9571 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56516  Script:$\{RESET\} $\{YELLOW\}$\{script_name\}$\{RESET\}$(printf "%$((40-$\{#script_name\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55358 \u56598  Model:$\{RESET\} $\{MAGENTA\}$\{model\}$\{RESET\}$(printf "%$((41-$\{#model\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u9201 \u65039   Total time:$\{RESET\} $\{GREEN\}$\{total_time\}s$\{RESET\}$(printf "%$((37-$\{#total_time\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  \
  if [ -n "$estimated_time" ]; then\
    local accuracy\
    accuracy=$(printf "%.1f" $(echo "scale=1; $estimated_time / $total_time * 100" | bc 2>/dev/null)) || accuracy="N/A"\
    echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56622  Est. vs Actual:$\{RESET\} $\{estimated_time\}s vs $\{total_time\}s ($\{accuracy\}%)$(printf "%$((21-$\{#estimated_time\}-$\{#total_time\}-$\{#accuracy\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  fi\
  \
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56580  API request time:$\{RESET\} $\{CYAN\}$\{api_time\}s$\{RESET\}$(printf "%$((31-$\{#api_time\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56589  Response parse time:$\{RESET\} $\{CYAN\}$\{parse_time\}s$\{RESET\}$(printf "%$((29-$\{#parse_time\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56541  Response size:$\{RESET\} ~$\{token_count\} words$(printf "%$((33-$\{#token_count\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{WHITE\}\u55357 \u56514  Script metrics:$\{RESET\}$(printf "%$((35))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  \
  local size_kb\
  size_kb=$(printf "%.2f" $(echo "scale=2; $\{script_size\}/1024" | bc 2>/dev/null)) || size_kb="N/A"\
  echo -e "$\{BLUE\}\uc0\u9553    $\{GRAY\}- Size: $\{script_size\} bytes ($\{size_kb\} KB)$\{RESET\}$(printf "%$((38-$\{#script_size\}-$\{#size_kb\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553    $\{GRAY\}- Lines: $\{script_lines\}$\{RESET\}$(printf "%$((43-$\{#script_lines\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553    $\{GRAY\}- Characters: $\{script_chars\}$\{RESET\}$(printf "%$((37-$\{#script_chars\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9568 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9571 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{GRAY\}\u55357 \u56523  Session ID: $\{SESSION_ID\}$\{RESET\}$(printf "%$((38-$\{#SESSION_ID\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{GRAY\}\u55357 \u56522  Benchmark logs: $\{BENCHMARK_DIR\}$\{RESET\}$(printf "%$((35-$\{#BENCHMARK_DIR\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{GRAY\}\u55357 \u56522  Metrics log: $\{METRICS_LOG\}$\{RESET\}$(printf "%$((38-$\{#METRICS_LOG\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{GRAY\}\u55357 \u56536  README file: $\{README\}$\{RESET\}$(printf "%$((38-$\{#README\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9553  $\{GRAY\}\u55357 \u56541  Changelog: $\{CHANGELOG\}$\{RESET\}$(printf "%$((39-$\{#CHANGELOG\}))s" "")$\{BLUE\}\u9553 $\{RESET\}"\
  echo -e "$\{BLUE\}\uc0\u9562 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9552 \u9565 $\{RESET\}"\
\}\
\
# Function to update the model performance database\
update_model_performance() \{\
  local model=$1\
  local file_size=$2\
  local duration=$3\
  \
  # Calculate time per KB\
  local time_per_kb=0\
  \
  # Prevent division by zero or invalid calculations\
  if [ -z "$file_size" ] || [ "$file_size" -le 0 ]; then\
    echo "Warning: Invalid file size ($file_size) for performance calculation" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
    return\
  fi\
  \
  if [ -z "$duration" ] || [[ ! "$duration" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || (( $(echo "$duration <= 0" | bc -l 2>/dev/null) )); then\
    echo "Warning: Invalid duration ($duration) for performance calculation" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
    return\
  fi\
  \
  # Calculate KB value first\
  local size_kb=$(echo "scale=4; $file_size / 1024" | bc 2>/dev/null)\
  \
  # Validate KB conversion\
  if [ -z "$size_kb" ] || [[ ! "$size_kb" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || (( $(echo "$size_kb <= 0" | bc -l 2>/dev/null) )); then\
    echo "Warning: Invalid KB conversion ($size_kb) for performance calculation" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
    return\
  fi\
  \
  # Calculate time per KB\
  time_per_kb=$(echo "scale=4; $duration / $size_kb" | bc 2>/dev/null)\
  \
  # Skip if the calculation failed or resulted in unreasonable values\
  if [ -z "$time_per_kb" ] || [[ ! "$time_per_kb" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || \
     (( $(echo "$time_per_kb <= 0" | bc -l 2>/dev/null) )) || \
     (( $(echo "$time_per_kb > 1000" | bc -l 2>/dev/null) )); then\
    echo "Warning: Unable to calculate reasonable performance metrics for $model (got $time_per_kb)" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
    return\
  fi\
  \
  echo "Calculated performance for $model: $time_per_kb secs/KB (file: $\{file_size\}B, $\{size_kb\}KB, duration: $\{duration\}s)" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
  \
  local perf_file="$\{BENCHMARK_DIR\}/model_performance.csv"\
  \
  # Create file if it doesn't exist\
  if [ ! -f "$perf_file" ]; then\
    echo "model,avg_time_per_kb,samples,last_updated" > "$perf_file"\
  fi\
  \
  # Check if model exists in performance file\
  if grep -q "^$\{model\}," "$perf_file"; then\
    # Read existing entry\
    local current_data=$(grep "^$\{model\}," "$perf_file")\
    \
    # Extract current values with validation\
    local current_avg=$(echo "$current_data" | cut -d, -f2)\
    if [ -z "$current_avg" ] || ! [[ "$current_avg" =~ ^[0-9]+(\\.[0-9]+)?$ ]]; then\
      echo "Warning: Invalid average time in database for $model. Resetting to current value." >> "$\{BENCHMARK_DIR\}/model_log.txt"\
      current_avg=$time_per_kb\
    fi\
    \
    local current_samples=$(echo "$current_data" | cut -d, -f3)\
    if [ -z "$current_samples" ] || ! [[ "$current_samples" =~ ^[0-9]+$ ]]; then\
      echo "Warning: Invalid sample count in database for $model. Resetting to 1." >> "$\{BENCHMARK_DIR\}/model_log.txt"\
      current_samples=1\
    fi\
    \
    # Calculate new average (weighted by number of samples)\
    local new_samples=$((current_samples + 1))\
    \
    # Ensure we have valid values for calculation\
    if [ "$current_samples" -gt 0 ] && [ -n "$current_avg" ] && [ -n "$time_per_kb" ]; then\
      local new_avg=$(echo "scale=4; (($current_avg * $current_samples) + $time_per_kb) / $new_samples" | bc 2>/dev/null)\
      \
      # Validate the calculated average\
      if [ -z "$new_avg" ] || ! [[ "$new_avg" =~ ^[0-9]+(\\.[0-9]+)?$ ]] || \
         (( $(echo "$new_avg <= 0" | bc -l 2>/dev/null) )) || \
         (( $(echo "$new_avg > 1000" | bc -l 2>/dev/null) )); then\
        echo "Warning: Invalid new average calculated for $model. Using current value." >> "$\{BENCHMARK_DIR\}/model_log.txt"\
        new_avg=$time_per_kb\
      fi\
      \
      # Update the entry\
      echo "Updating model performance: $model, old: $current_avg ($current_samples samples), new: $new_avg ($new_samples samples)" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
      sed -i '' "s/^$\{model\},$\{current_avg\},$\{current_samples\},.*/$\{model\},$\{new_avg\},$\{new_samples\},$(date '+%Y-%m-%d %H:%M:%S')/" "$perf_file"\
    else\
      # If we have invalid data, just replace with new data\
      echo "Warning: Invalid data for weighted average calculation. Using current value for $model." >> "$\{BENCHMARK_DIR\}/model_log.txt"\
      sed -i '' "s/^$\{model\},.*/$\{model\},$\{time_per_kb\},1,$(date '+%Y-%m-%d %H:%M:%S')/" "$perf_file"\
    fi\
  else\
    # Add new entry\
    echo "Adding new model performance entry for $model: $time_per_kb secs/KB" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
    echo "$\{model\},$\{time_per_kb\},1,$(date '+%Y-%m-%d %H:%M:%S')" >> "$perf_file"\
  fi\
  \
  # Log the update\
  echo "$(date '+%Y-%m-%d %H:%M:%S') - Updated performance metrics for $model: $time_per_kb secs/KB (file size: $\{file_size\}B, duration: $\{duration\}s)" >> "$\{BENCHMARK_DIR\}/model_log.txt"\
\}\
\
# Function to update the changelog\
update_changelog() \{\
  local script_name=$1\
  local model=$2\
  local duration=$3\
  \
  # Get the current date and time\
  local today=$(date '+%Y-%m-%d')\
  local timestamp=$(date '+%Y-%m-%d %H:%M:%S')\
  \
  # Check if there's already an entry for today\
  if grep -q "### $\{today\}" "$\{CHANGELOG\}"; then\
    # Append to today's entry with timestamp\
    sed -i '' "/### $\{today\}/a\\\\\
- Generated README for $\{script_name\} with $\{model\} ($\{duration\}s) at $\{timestamp\}\
" "$\{CHANGELOG\}"\
  else\
    # Create a new entry for today with timestamp\
    sed -i '' "/^## Version/a\\\\\
\\\\\
### $\{today\}\\\\\
- Generated README for $\{script_name\} with $\{model\} ($\{duration\}s) at $\{timestamp\}\
" "$\{CHANGELOG\}"\
  fi\
  \
  # Log to console\
  echo -e "$\{GREEN\}Updated changelog at $\{CHANGELOG\} with entry for $\{script_name\}$\{RESET\}"\
\}\
\
# Function to check if running in a terminal\
is_terminal() \{\
  [ -t 0 ]\
\}\
\
# Function to count script features\
count_script_features() \{\
  local content="$1"\
  \
  # Count functions (very basic, may need refinement)\
  local function_count=$(echo "$\{content\}" | grep -c "function " || echo "0")\
  \
  # Count commands (very rough estimate)\
  local command_count=$(echo "$\{content\}" | grep -v "^#" | grep -v "^$" | grep -c ";" || echo "0")\
  \
  # Count conditionals\
  local if_count=$(echo "$\{content\}" | grep -c "if " || echo "0")\
  local case_count=$(echo "$\{content\}" | grep -c "case " || echo "0")\
  \
  # Count loops\
  local for_count=$(echo "$\{content\}" | grep -c "for " || echo "0")\
  local while_count=$(echo "$\{content\}" | grep -c "while " || echo "0")\
  \
  # Add features to metrics log (with error handling)\
  jq --arg functions "$\{function_count\}" \\\
     --arg commands "$\{command_count\}" \\\
     --arg ifs "$\{if_count\}" \\\
     --arg cases "$\{case_count\}" \\\
     --arg fors "$\{for_count\}" \\\
     --arg whiles "$\{while_count\}" \\\
     '.script_features = \{"function_count": $functions, "command_count": $commands, "if_count": $ifs, "case_count": $cases, "for_count": $fors, "while_count": $whiles\}' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with script features"\
  fi\
\}\
\
# Function to check for dependencies\
check_dependencies() \{\
  local missing_deps=0\
  \
  # Start timer\
  local start_time=$(date +%s.%N)\
  \
  # Check for jq\
  if ! command -v jq &> /dev/null; then\
    echo "Error: jq is required. Please install jq (e.g., 'brew install jq')."\
    missing_deps=1\
  fi\
  \
  # Check for bc\
  if ! command -v bc &> /dev/null; then\
    echo "Error: bc is required. Please install bc."\
    missing_deps=1\
  fi\
  \
  # Check for ollama\
  if ! command -v ollama &> /dev/null; then\
    echo "Error: ollama is required. Please install Ollama."\
    missing_deps=1\
  else\
    # Check if Ollama server is running\
    if ! curl -s -m 2 "http://localhost:11434/api/tags" &> /dev/null; then\
      echo "Error: Ollama server is not running. Please start it with 'ollama serve'."\
      missing_deps=1\
    fi\
  fi\
  \
  # End timer\
  local end_time=$(date +%s.%N)\
  local duration=$(printf "%.2f" $(echo "$\{end_time\} - $\{start_time\}" | bc))\
  \
  log_benchmark "system" "0" "0" "0" "system" "dependency_check" "$\{duration\}" "0"\
  \
  return $\{missing_deps\}\
\}\
\
# Function to get available models and determine their complexity\
get_models() \{\
  local start_time=$(date +%s.%N)\
  \
  echo "Fetching available models..."\
  local ollama_output=$(ollama list 2>/dev/null)\
  \
  if [ $? -ne 0 ]; then\
    echo "Error: Failed to run 'ollama list'. Ensure Ollama is running."\
    exit 1\
  fi\
  \
  # Parse models\
  models=($(echo "$ollama_output" | tail -n +2 | awk '\{print $1\}'))\
  model_ids=($(echo "$ollama_output" | tail -n +2 | awk '\{print $2\}'))\
  model_sizes=($(echo "$ollama_output" | tail -n +2 | awk '\{print $3, $4\}'))\
  \
  if [ $\{#models[@]\} -eq 0 ]; then\
    echo "No models found. Please download a model using 'ollama pull <model>'."\
    exit 1\
  fi\
  \
  # Estimate complexity based on model info\
  for ((i=1; i<=$\{#models[@]\}; i++)); do\
    local model_name="$\{models[$i]\}"\
    local model_size="$\{model_sizes[$i]\}"\
    local size_number=0\
    local complexity=0\
    \
    # Try to extract parameter size from model name\
    # Common formats: model-7b, model:7b, model/7b, model-7B, etc.\
    if [[ "$model_name" =~ ([0-9]+)[bB] ]]; then\
      size_number=$\{BASH_REMATCH[1]\}\
    fi\
    \
    # If not found in name, check the model size info from ollama list\
    if [ "$size_number" -eq 0 ] && [[ "$model_size" =~ ([0-9]+(\\.[0-9]+)?)B ]]; then\
      size_number=$\{BASH_REMATCH[1]\}\
    fi\
    \
    # If still not found, try parameter_size from model info\
    if [ "$size_number" -eq 0 ]; then\
      # Get model info to extract parameter size\
      local model_info=$(ollama show $model_name 2>/dev/null)\
      if [[ "$model_info" =~ 'parameter_size":' ]]; then\
        if [[ "$model_info" =~ 'parameter_size":"([0-9\\.]+)B' ]]; then\
          size_number=$\{BASH_REMATCH[1]\}\
        fi\
      fi\
    fi\
    \
    # Assign complexity based on size if found\
    if (( $(echo "$size_number > 0" | bc -l 2>/dev/null) )); then\
      # Determine size category\
      if (( $(echo "$size_number < 4" | bc -l 2>/dev/null) )); then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["1-3B"]\}\
      elif (( $(echo "$size_number < 8" | bc -l 2>/dev/null) )); then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["4-7B"]\}\
      elif (( $(echo "$size_number < 14" | bc -l 2>/dev/null) )); then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["8-13B"]\}\
      else\
        complexity=$\{MODEL_SIZE_COMPLEXITY["14B+"]\}\
      fi\
    else\
      # If size can't be determined, use default and adjust based on name heuristics\
      complexity=$\{MODEL_COMPLEXITY["default"]\}\
      \
      # Adjust based on common model family naming conventions\
      if [[ "$model_name" =~ (tiny|small|lite|1b|2b|3b) ]]; then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["1-3B"]\}\
      elif [[ "$model_name" =~ (base|7b|6b|5b|4b) ]]; then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["4-7B"]\}\
      elif [[ "$model_name" =~ (large|13b|12b|11b|10b|9b|8b) ]]; then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["8-13B"]\}\
      elif [[ "$model_name" =~ (xl|xxl|huge|70b|35b|34b|33b|32b|31b|30b|20b|19b|18b|17b|16b|15b|14b) ]]; then\
        complexity=$\{MODEL_SIZE_COMPLEXITY["14B+"]\}\
      fi\
    fi\
    \
    # Store complexity\
    MODEL_COMPLEXITY["$model_name"]=$complexity\
    \
    # Log model details to log file\
    echo "Model: $model_name, Size: $model_size, Complexity Factor: $complexity" >> "$\{BENCHMARK_DIR\}/models_log.txt"\
  done\
  \
  # Set default model to first available model (unless already set)\
  if [ -z "$DEFAULT_MODEL" ]; then\
    DEFAULT_MODEL="$\{models[1]\}"\
  fi\
  \
  # Add models to metrics log\
  local models_json="["\
  for ((i=1; i<=$\{#models[@]\}; i++)); do\
    models_json+="\\"$\{models[$i]\}\\""\
    if [ $i -lt $\{#models[@]\} ]; then\
      models_json+=","\
    fi\
  done\
  models_json+="]"\
  \
  jq --argjson models "$\{models_json\}" '.available_models = $models' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with available models"\
  fi\
  \
  local end_time=$(date +%s.%N)\
  local duration=$(printf "%.2f" $(echo "$\{end_time\} - $\{start_time\}" | bc))\
  \
  log_benchmark "system" "0" "0" "0" "system" "fetch_models" "$\{duration\}" "$\{#models[@]\}"\
  \
  # Just list models if that's what was requested\
  if [[ "$1" == "--list" ]]; then\
    echo "Available Ollama models for README generation:"\
    printf "%-30s %-15s %-15s\\n" "MODEL" "SIZE" "EST. SPEED"\
    printf "%-30s %-15s %-15s\\n" "-----" "----" "---------"\
    for ((i=1; i<=$\{#models[@]\}; i++)); do\
      local model_name="$\{models[$i]\}"\
      local model_size="$\{model_sizes[$i]\}"\
      \
      # Get complexity factor for speed estimation\
      local complexity=$\{MODEL_COMPLEXITY[$model_name]\}\
      if [ -z "$complexity" ]; then\
        complexity=$\{MODEL_COMPLEXITY["default"]\}\
      fi\
      \
      # Calculate relative speed\
      local speed=""\
      if (( $(echo "$complexity < 1.5" | bc -l 2>/dev/null) )); then\
        speed="Very Fast"\
      elif (( $(echo "$complexity < 2.5" | bc -l 2>/dev/null) )); then\
        speed="Fast"\
      elif (( $(echo "$complexity < 4.0" | bc -l 2>/dev/null) )); then\
        speed="Medium"\
      else\
        speed="Slow"\
      fi\
      \
      printf "%-30s %-15s %-15s\\n" "$model_name" "$model_size" "$speed"\
    done\
    exit 0\
  fi\
  \
  echo "Available models: $\{models[@]\}"\
\}\
\
# Function to select a model\
select_model() \{\
  local start_time=$(date +%s.%N)\
  \
  # If a model is provided as an argument, use it\
  if [ $# -ge 2 ]; then\
    local provided_model="$2"\
    # Check if the model is available\
    if ollama list | grep -q "$\{provided_model\}"; then\
      model="$\{provided_model\}"\
    else\
      echo "Warning: Model '$provided_model' not found in available models."\
      echo "Pulling the model..."\
      ollama pull "$\{provided_model\}"\
      if [ $? -ne 0 ]; then\
        echo "Error: Failed to pull model '$\{provided_model\}'. Using default model: $\{DEFAULT_MODEL\}"\
        model="$\{DEFAULT_MODEL\}"\
      else\
        model="$\{provided_model\}"\
      fi\
    fi\
  elif is_terminal; then\
    # Interactive selection if terminal is available\
    echo "Select a model for README generation:"\
    printf "%-5s %-30s %-15s %-15s\\n" "NUM" "MODEL" "SIZE" "EST. SPEED"\
    printf "%-5s %-30s %-15s %-15s\\n" "---" "-----" "----" "---------"\
    for ((i=1; i<=$\{#models[@]\}; i++)); do\
      local model_name="$\{models[$i]\}"\
      local model_size="$\{model_sizes[$i]\}"\
      \
      # Get complexity factor for speed estimation\
      local complexity=$\{MODEL_COMPLEXITY[$model_name]\}\
      if [ -z "$complexity" ]; then\
        complexity=$\{MODEL_COMPLEXITY["default"]\}\
      fi\
      \
      # Calculate relative speed\
      local speed=""\
      if (( $(echo "$complexity < 1.5" | bc -l) )); then\
        speed="Very Fast"\
      elif (( $(echo "$complexity < 2.5" | bc -l) )); then\
        speed="Fast"\
      elif (( $(echo "$complexity < 4.0" | bc -l) )); then\
        speed="Medium"\
      else\
        speed="Slow"\
      fi\
      \
      printf "%-5s %-30s %-15s %-15s\\n" "$i" "$model_name" "$model_size" "$speed"\
    done\
    echo ""\
    echo "Enter the number of the model to use:"\
    read model_num\
    \
    if [[ "$model_num" =~ ^[0-9]+$ ]] && [ "$model_num" -ge 1 ] && [ "$model_num" -le $\{#models[@]\} ]; then\
      model="$\{models[$model_num]\}"\
    else\
      echo "Invalid selection. Using default model: $\{DEFAULT_MODEL\}"\
      model="$\{DEFAULT_MODEL\}"\
    fi\
  else\
    # Non-interactive: use the default model\
    model="$\{DEFAULT_MODEL\}"\
    echo "No terminal available for interactive selection. Using default model: $\{model\}"\
  fi\
  \
  local end_time=$(date +%s.%N)\
  local duration=$(printf "%.2f" $(echo "$\{end_time\} - $\{start_time\}" | bc))\
  \
  log_benchmark "$\{INPUT\}" "0" "0" "0" "$\{model\}" "model_selection" "$\{duration\}" "0"\
  \
  # Record the selected model\
  jq --arg model "$\{model\}" '.selected_model = $model' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with selected model"\
  fi\
  \
  echo "Selected model: $\{model\}"\
\}\
\
# Function to validate input file\
validate_input_file() \{\
  local input="$1"\
  local start_time=$(date +%s.%N)\
  \
  if [ ! -f "$\{input\}" ]; then\
    echo "Error: Input file '$\{input\}' does not exist."\
    exit 1\
  fi\
  \
  # Get file stats\
  local file_size=$(stat -f%z "$\{input\}")\
  CONTENT=$(cat "$\{input\}")\
  local line_count=$(echo "$\{CONTENT\}" | wc -l | tr -d ' ')\
  local char_count=$(echo "$\{CONTENT\}" | wc -c | tr -d ' ')\
  \
  # Record file metrics\
  jq --arg file "$\{input\}" \\\
     --arg size "$\{file_size\}" \\\
     --arg lines "$\{line_count\}" \\\
     --arg chars "$\{char_count\}" \\\
     '.input_file = \{"path": $file, "size_bytes": $size, "line_count": $lines, "char_count": $chars\}' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
     \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with file metrics"\
  fi\
  \
  # Determine file extension\
  ext="$\{input##*.\}"\
  \
  # Check supported file types\
  case "$\{ext\}" in\
    sh|bash|zsh)\
      # Check if it's base64 encoded (improved detection with better pattern matching)\
      # More carefully detect base64 content by counting valid character ratio\
      local total_chars=$(echo -n "$CONTENT" | wc -c)\
      local valid_chars=$(echo -n "$CONTENT" | tr -cd 'A-Za-z0-9+/=' | wc -c)\
      local ratio=$((valid_chars * 100 / total_chars))\
      \
      # If at least 90% of characters are valid base64 characters, try decoding\
      if [ "$ratio" -ge 90 ]; then\
        # Clean the content by removing any non-base64 characters\
        local CLEANED_CONTENT=$(echo "$CONTENT" | tr -cd 'A-Za-z0-9+/=')\
        \
        # Add padding if needed (ensure length is multiple of 4 by adding '=' characters)\
        local remainder=$(($\{#CLEANED_CONTENT\} % 4))\
        if [ "$remainder" -ne 0 ]; then\
          local padding=$((4 - remainder))\
          for ((i=0; i<padding; i++)); do\
            CLEANED_CONTENT="$\{CLEANED_CONTENT\}="\
          done\
        fi\
        \
        # Attempt to decode as base64\
        local DECODED_CONTENT=$(echo "$CLEANED_CONTENT" | base64 -d 2>/dev/null)\
        local DECODE_STATUS=$?\
        \
        # Check if decode was successful\
        if [ $DECODE_STATUS -eq 0 ] && [ -n "$DECODED_CONTENT" ]; then\
          # Check for various script identifiers, not just shell scripts\
          if echo "$DECODED_CONTENT" | grep -qE "^(#!/|#\\s*encoding:|import |function |def |class |public class|<\\?php)"; then\
            CONTENT="$DECODED_CONTENT"\
            echo "Input detected as base64-encoded script. Decoded successfully."\
          else\
            # Try to determine if it's valid text content at least\
            if echo "$DECODED_CONTENT" | head -c 1000 | LC_ALL=C grep -q "^[ -~\\t\\n\\r]*$"; then\
              CONTENT="$DECODED_CONTENT"\
              echo "Input detected as base64-encoded text. Decoded successfully but not recognized as a standard script format."\
            else\
              echo "Input appears to be base64 but decoded content is not valid script or text. Treating as plain text."\
            fi\
          fi\
        else\
          echo "Input has base64-like characters but failed to decode properly. Treating as plain text."\
        fi\
      fi\
      \
      # Validate as shell script\
      if ! echo "$\{CONTENT\}" | grep -qE "^#!/bin/(bash|zsh|sh)"; then\
        echo "Warning: Input does not appear to be a valid shell script (missing shebang)."\
      fi\
      SCRIPT_TYPE="shell"\
      ;;\
    scpt|applescript)\
      # AppleScript\
      # If it's a compiled .scpt file, try to decompile it\
      if [[ "$\{ext\}" == "scpt" ]]; then\
        echo "Detected compiled AppleScript file. Attempting to decompile..."\
        local DECOMPILED_CONTENT=$(osadecompile "$\{input\}" 2>/dev/null)\
        if [ $? -eq 0 ]; then\
          CONTENT="$DECOMPILED_CONTENT"\
          echo "Successfully decompiled .scpt file to readable AppleScript."\
        else\
          echo "Warning: Failed to decompile .scpt file. Will analyze binary content."\
        fi\
      fi\
      SCRIPT_TYPE="applescript"\
      ;;\
    py|python)\
      # Python script\
      SCRIPT_TYPE="python"\
      ;;\
    rb|ruby)\
      # Ruby script\
      SCRIPT_TYPE="ruby"\
      ;;\
    js|javascript)\
      # JavaScript\
      SCRIPT_TYPE="javascript"\
      ;;\
    *)\
      echo "Warning: Unsupported file type: $\{ext\}. Will attempt to analyze as generic script."\
      SCRIPT_TYPE="generic"\
      ;;\
  esac\
  \
  # Count script features\
  count_script_features "$\{CONTENT\}"\
  \
  local end_time=$(date +%s.%N)\
  local duration=$(printf "%.2f" $(echo "$\{end_time\} - $\{start_time\}" | bc))\
  \
  log_benchmark "$\{input\}" "$\{file_size\}" "$\{line_count\}" "$\{char_count\}" "$\{model\}" "file_validation" "$\{duration\}" "0"\
  \
  # Store file metrics for later use\
  FILE_SIZE="$\{file_size\}"\
  LINE_COUNT="$\{line_count\}"\
  CHAR_COUNT="$\{char_count\}"\
  \
  echo "File validated: $\{input\} ($\{file_size\} bytes, $\{line_count\} lines)"\
\}\
\
# Function to generate README from script\
generate_readme() \{\
  local input="$1"\
  local model="$2"\
  local skip_estimate="$3"\
  local script_basename=$(basename "$\{input\}")\
  local start_time=$(date +%s.%N)\
  \
  echo -e "$\{BLUE\}Generating README documentation for $\{YELLOW\}$\{script_basename\}$\{RESET\} with $\{MAGENTA\}$\{model\}$\{BLUE\}...$\{RESET\}"\
  \
  # Estimate completion time if not skipped\
  local estimated_seconds=""\
  if [ "$skip_estimate" != "true" ]; then\
    # Run the estimation and ensure it's a valid number\
    estimated_seconds=$(estimate_completion_time "$\{FILE_SIZE\}" "$\{model\}" "$\{LINE_COUNT\}")\
    if ! [[ "$estimated_seconds" =~ ^[0-9]+$ ]]; then\
      echo "Warning: Invalid estimate received: '$\{estimated_seconds\}'. Using default of 30 seconds."\
      estimated_seconds=30\
    fi\
    local estimated_time=$(format_time "$\{estimated_seconds\}")\
    echo "Estimated completion time: $\{estimated_time\}"\
    jq --arg est "$\{estimated_seconds\}" '.estimated_completion_time = $est' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
    \
    # Check if jq succeeded\
    if [ $? -eq 0 ]; then\
      mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
    else\
      echo "Warning: Failed to update metrics log with estimated completion time"\
    fi\
  fi\
  \
  # Create request payload - using a more robust approach with enhanced prompts\
  local system_prompt="You are an expert code documentarian tasked with producing professional, accurate, and comprehensive documentation. Analyze the provided $\{SCRIPT_TYPE\} script with precision, describing only the functionality explicitly present in the code. Your ONLY task is to generate a detailed Markdown README in the exact format requested below - follow the section structure exactly as specified in the user's instructions. Generate documentation that is clear, thorough, and professionally structured, suitable for developers and end-users. Do not include document titles or dates in your response as they will be added separately.\
\
Your documentation must be high quality with proper markdown formatting including:\
- Table of contents with anchor links to all sections (e.g., [Overview](#overview))\
- Clear section headers using proper markdown heading levels (##, ###)\
- Code blocks with language-specific syntax highlighting (e.g., \\`\\`\\`bash)\
- Tables for structured data when appropriate using proper markdown table syntax\
- Bullet points and numbered lists for better readability\
- Bold (**text**) and italic (*text*) formatting for emphasis where needed\
\
Make sure your documentation is comprehensive, covering all aspects of the script with attention to detail. Focus on providing practical, actionable information that would help users understand and use the script effectively. Include specific examples and command snippets wherever possible."\
\
  local user_prompt="Analyze the following $\{SCRIPT_TYPE\} script provided as plain text. Pay close attention to specific elements such as references to applications, system paths, and command-line tools. Consider the script's potential impact on the system.\
\
Generate a high-quality, comprehensive Markdown README with these mandatory sections:\
\
## Table of Contents\
- Create a proper table of contents with anchor links to all sections\
- Format as [Section Name](#section-name-lowercase-with-hyphens)\
\
## Overview\
- Provide a detailed summary of the script's purpose\
- Explain its significance and value to users\
- Mention the context in which it's useful (for what types of users or situations)\
- Include a brief description of what specific problem the script solves\
\
## Requirements\
- List ALL prerequisites directly inferred from the script\
- Include required software, dependencies, and minimum system requirements\
- Specify exact version numbers when possible (e.g., Bash 4.0+, jq 1.6+)\
- Format as a bulleted list with proper markdown syntax\
\
## Installation\
- Offer comprehensive step-by-step instructions\
- Number each step sequentially \
- Include commands in code blocks with syntax highlighting\
- Cover all installation scenarios (e.g., different operating systems)\
- If no explicit installation is needed, provide guidance on downloading and setting permissions\
\
## Usage\
- Include complete syntax: \\`./scriptname.sh [options] <arguments>\\`\
- List ALL available options/flags with their descriptions\
- Show multiple example commands for different use cases\
- Display sample outputs where helpful\
- Format command examples in code blocks with bash syntax highlighting\
\
## Configuration\
- Explain ALL configuration options and settings\
- Use tables to present configuration options with columns for: Option Name | Default Value | Description\
- Show examples of how to modify configuration values\
- Include information about config file locations if applicable\
\
## What the Script Does\
- Describe the script's operations step-by-step in detail\
- Use subsections with proper heading levels (###) for each major function\
- Include code snippets to illustrate key operations\
- Explain the purpose of each major function or block\
- Discuss how data flows through the script\
- Include diagrams or flowcharts if helpful (using markdown syntax)\
\
## Important Notes\
- Highlight ALL critical details derived from the script\
- Use bold (**text**) for warnings and important information\
- Use italic (*text*) for emphasis of secondary points\
- Include notes about limitations, edge cases, or potential issues\
\
## Troubleshooting\
- Include at least 5 common issues and their detailed solutions\
- Format as a problem/solution section with clear headings\
- Include troubleshooting commands in code blocks\
- Suggest how to interpret error messages\
- Provide debugging tips\
\
## Disclaimer\
- Warn about ALL potential risks of running the script\
- Include clear statements about data safety and system impact\
- Mention any licensing or usage restrictions\
- Include recommendations for backing up data before use\
\
Do NOT add any top-level title headers like \\"# Documentation\\" or dates - these will be added separately. Make sure your documentation follows best practices for markdown formatting and provides comprehensive, specific information with actual command examples and detailed explanations. Avoid vague or general statements - be precise and thorough.\
\
File: $\{script_basename\}\
\
Script size metrics:\
- Size: $\{FILE_SIZE\} bytes\
- Lines: $\{LINE_COUNT\}\
- Characters: $\{CHAR_COUNT\}\
\
Script Content:\
$\{CONTENT\}"\
\
  # Create temporary files for the prompts\
  local system_file=$(mktemp)\
  local user_file=$(mktemp)\
  \
  # Write prompts to JSON-compatible files\
  printf "%s" "$system_prompt" | jq -R -s '.' > "$system_file"\
  printf "%s" "$user_prompt" | jq -R -s '.' > "$user_file"\
  \
  # Use jq to properly handle inputs and special characters\
  local payload=$(jq -n \\\
    --arg model "$\{model\}" \\\
    --rawfile system_content "$system_file" \\\
    --rawfile user_content "$user_file" \\\
    '\{\
      "model": $model,\
      "messages": [\
        \{\
          "role": "system",\
          "content": $system_content\
        \},\
        \{\
          "role": "user", \
          "content": $user_content\
        \}\
      ],\
      "stream": false\
    \}')\
    \
  # Clean up temporary files\
  rm "$system_file" "$user_file"\
  \
  # Log the prompt size (in characters)\
  local prompt_size=$(echo "$\{payload\}" | jq -r '.messages[1].content' 2>/dev/null | wc -c | tr -d ' ')\
  jq --arg size "$\{prompt_size\}" '.prompt_size_chars = $size' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with prompt size"\
  fi\
  \
  # Log the payload for debugging\
  echo "$\{payload\}" > "$\{BENCHMARK_DIR\}/request_payload_$\{SESSION_ID\}.json"\
\
  # Send request to Ollama API\
  echo -e "$\{BLUE\}Sending request to Ollama API...$\{RESET\}"\
  local request_start_time=$(date +%s.%N)\
  \
  # Temporary file for response\
  local temp_response=$(mktemp)\
  \
  # Set a generous timeout value (15 minutes max)\
  local max_timeout=900\
  \
  # Log the original model\
  echo -e "$\{BLUE\}Using model: $\{MAGENTA\}$\{model\}$\{RESET\}"\
  \
  # Apply enhanced prompting to all models for better README quality\
  echo -e "$\{YELLOW\}Note: Applying enhanced README generation instructions$\{RESET\}"\
  \
  # Apply model-specific adjustments\
  if [[ "$model" == "deepseek-coder:latest" ]]; then\
    echo -e "$\{CYAN\}Applying deepseek-coder-specific prompt optimizations...$\{RESET\}"\
  elif [[ "$model" == *"codellama"* ]]; then\
    echo -e "$\{CYAN\}Applying codellama-specific prompt optimizations...$\{RESET\}"\
  fi\
  \
  # Enhance the system prompt with stronger format and content requirements inspired by qwen2.5-coder:7b output\
  system_prompt="You are an expert code documentarian tasked with producing professional, comprehensive, and detailed documentation. Your documentation must be structured, clear, and insightful, closely analyzing the provided script. The README you generate must follow these exact requirements:\
\
1. CONTENT REQUIREMENTS:\
   - Provide a THOROUGH and DETAILED analysis of the script's functionality\
   - Include specific examples from the code when explaining features\
   - Use numbered lists to break down complex processes\
   - Highlight important functions with bold formatting\
   - Present information in a well-structured, easy-to-understand format\
   - Include specific technical details that demonstrate deep understanding of the code\
   - MINIMUM 500 WORDS for the main analysis section\
   - For each major function, provide at least 2-3 sentences of explanation\
\
2. FORMATTING REQUIREMENTS:\
   - Your documentation must be properly formatted in Markdown\
   - Use proper heading levels (## for main sections, ### for subsections)\
   - Make liberal use of formatting including **bold**, *italic*, code blocks with syntax highlighting, and lists\
   - Create well-formatted tables when presenting structured data\
   - Format code samples with proper ```bash syntax\
   - Use bullet points for feature lists\
   - Format all section headers consistently\
\
3. STRUCTURE GUIDANCE:\
   Start with a comprehensive overview paragraph that summarizes the script's purpose and functionality.\
   Then create a structured analysis with numbered points that break down:\
   1. Input handling and parameter processing\
   2. System information collection\
   3. Dependency verification\
   4. Core functionality analysis\
   5. Output and logging mechanisms\
   6. Performance considerations\
\
   DO NOT output a single paragraph summary. The analysis MUST use numbered points with detailed explanations.\
\
4. QUALITY EXPECTATIONS:\
   - Be specific rather than generic\
   - Include technical details rather than general descriptions\
   - Focus on functionality evident in the code, not assumptions\
   - Match the depth and quality of this example:\
\
Example quality level:\
\\"This script appears to be an automated analysis tool for analyzing scripts in a technical context. It includes several key functions:\
\
1. **Input Handling**: The script accepts input parameters, validates them, and sets up the environment based on user inputs. It processes command-line arguments including options like --debug, --help, and --version, validating that an input file is provided.\
\
2. **System Information Collection**: It gathers system information which could be useful for performance metrics or debugging purposes. This includes CPU details, memory capacity, OS version, and Ollama version using commands like sysctl and sw_vers.\
\
3. **Dependency Check**: Before proceeding with analysis, it checks if all necessary dependencies are installed and available. This includes verifying jq, bc, and Ollama are installed, and that the Ollama server is running properly.\
\
4. **Model Selection and Analysis**: The script supports selecting a model to analyze the input file. It identifies available models, allows interactive selection in a terminal environment, and supports direct model specification via command line arguments.\\"\
\
5. SECTION REQUIREMENTS:\
   Your documentation MUST include these exact sections in this exact order:\
   \
   ## Overview\
   (Comprehensive summary paragraph)\
   \
   ## Detailed Analysis\
   (Numbered list of 5+ key components)\
   \
   ## Requirements\
   (List of prerequisites and dependencies)\
   \
   ## Usage\
   (Command format and examples)\
\
IMPORTANT: Your goal is to create a high-quality technical document that helps readers understand exactly what the script does and how it functions. Match or exceed the quality of the example above. If your output is a single paragraph or lacks numbered points, it will be considered incorrect."\
  fi\
  \
  # Show progress bar if estimate is available\
  if [ -n "$estimated_seconds" ]; then\
    # Ensure generous timeout - use estimated time + 200% buffer or max_timeout, whichever is smaller\
    local curl_timeout=$((estimated_seconds * 3))\
    if [ $curl_timeout -gt $max_timeout ]; then\
      curl_timeout=$max_timeout\
    fi\
    \
    # Make sure we never have less than 5 minutes for any model\
    if [ $curl_timeout -lt 300 ]; then\
      curl_timeout=300\
    fi\
    \
    echo "Setting API request timeout to $\{curl_timeout\}s"\
    \
    # Send the request in background with timeout - with simplified options\
    curl -s -m $curl_timeout -X POST "$\{OLLAMA_API\}" \\\
      -H "Content-Type: application/json" \\\
      -d "$\{payload\}" > "$\{temp_response\}" 2>>"$\{BENCHMARK_DIR\}/curl_debug_$\{SESSION_ID\}.log" &\
    \
    local pid=$!\
    local start_secs=$SECONDS\
    local progress=0\
    \
    # Show progress while curl is running\
    while kill -0 $pid 2>/dev/null; do\
      local elapsed=$((SECONDS - start_secs))\
      \
      # Check if we've exceeded the maximum timeout\
      if [ $elapsed -gt $max_timeout ]; then\
        echo ""\
        echo "Error: API request exceeded maximum timeout of $\{max_timeout\}s."\
        echo "Terminating request to $\{model\}..."\
        kill -9 $pid 2>/dev/null\
        echo "\{\\"error\\": \\"Request timeout after $\{max_timeout\} seconds.\\"\}" > "$\{temp_response\}"\
        break\
      fi\
      \
      if [ $estimated_seconds -gt 0 ]; then\
        progress=$((elapsed * 100 / estimated_seconds))\
        # Cap at 99% until complete\
        if [ $progress -gt 99 ]; then\
          progress=99\
        fi\
      else\
        progress=50  # Default to 50% if no estimate\
      fi\
      \
      display_progress $progress "$(format_time $elapsed)"\
      sleep 0.5\
    done\
    \
    # Check if curl completed successfully\
    wait $pid\
    local curl_status=$?\
    \
    # Handle different exit status codes\
    if [ $curl_status -ne 0 ]; then\
      echo ""\
      \
      # If we have our custom timeout error, that's already handled\
      if [ -s "$\{temp_response\}" ] && grep -q "Request timeout" "$\{temp_response\}"; then\
        # This is fine, continue processing\
        :\
      # If we terminated it with SIGTERM (143) due to our timeout, that's expected\
      elif [ $curl_status -eq 143 ]; then\
        # This is our handled timeout, continue\
        :\
      # For curl timeout (exit code 28)\
      elif [ $curl_status -eq 28 ]; then\
        echo "Error: Request to Ollama API timed out after $\{curl_timeout\}s."\
        echo "\{\\"error\\": \\"Curl timeout after $\{curl_timeout\} seconds (exit code 28).\\"\}" > "$\{temp_response\}"\
      # Other curl errors\
      else\
        if ! [ -s "$\{temp_response\}" ]; then\
          echo "Error: Failed to connect to Ollama API (curl exit code: $\{curl_status\})."\
          echo "Please ensure Ollama server is running with: ollama serve"\
          echo "\{\\"error\\": \\"Connection failed to Ollama API (curl exit code: $\{curl_status\}).\\"\}" > "$\{temp_response\}"\
        else\
          echo "Error: Request to Ollama API failed (curl exit code: $\{curl_status\})."\
          if ! grep -q "error" "$\{temp_response\}"; then\
            # Add error message to response if not present\
            echo "\{\\"error\\": \\"Request failed (curl exit code: $\{curl_status\}).\\"\}" > "$\{temp_response\}"\
          fi\
        fi\
      fi\
    fi\
    \
    # Show 100% completion if successful\
    if [ -s "$\{temp_response\}" ] && ! grep -q "error" "$\{temp_response\}"; then\
      display_progress 100 "$(format_time $((SECONDS - start_secs)))"\
      echo ""\
    fi\
  else\
    # Regular request without progress bar but with timeout\
    local curl_output=$(curl -v -s -m $max_timeout -X POST "$\{OLLAMA_API\}" \\\
      -H "Content-Type: application/json" \\\
      -d "$\{payload\}" 2>&1)\
    local curl_status=$?\
    \
    # Write the output to temp file\
    echo "$curl_output" > "$\{temp_response\}"\
    \
    # Check if curl failed\
    if [ $curl_status -ne 0 ]; then\
      echo "Error: API request to $\{model\} failed with status code $curl_status."\
      case $curl_status in\
        28)\
          echo "Request timed out after $\{max_timeout\}s."\
          echo "\{\\"error\\": \\"Request to $\{model\} timed out after $\{max_timeout\} seconds (curl exit code: 28).\\"\}" > "$\{temp_response\}"\
          ;;\
        7)\
          echo "Failed to connect to Ollama API. Please ensure Ollama server is running with: ollama serve"\
          echo "\{\\"error\\": \\"Failed to connect to Ollama API (curl exit code: 7).\\"\}" > "$\{temp_response\}"\
          ;;\
        *)\
          echo "Request failed with curl exit code: $curl_status. See error details for more information."\
          echo "\{\\"error\\": \\"Request to $\{model\} failed (curl exit code: $curl_status).\\"\}" > "$\{temp_response\}"\
          ;;\
      esac\
    fi\
    \
    # Check if response is valid JSON with content\
    if ! grep -q '"content"' "$\{temp_response\}" && ! grep -q '"error"' "$\{temp_response\}"; then\
      echo "Warning: Response may not contain expected content. Will try to parse anyway."\
    fi\
  fi\
  \
  local request_end_time=$(date +%s.%N)\
  local request_duration=$(printf "%.2f" $(echo "$\{request_end_time\} - $\{request_start_time\}" | bc))\
  \
  log_benchmark "$\{script_basename\}" "$\{FILE_SIZE\}" "$\{LINE_COUNT\}" "$\{CHAR_COUNT\}" "$\{model\}" "api_request" "$\{request_duration\}" "$\{prompt_size\}"\
  \
  # Parse response\
  echo -e "$\{BLUE\}Processing response...$\{RESET\}"\
  local parse_start_time=$(date +%s.%N)\
  \
  # Check if response exists and has content\
  if [ ! -s "$\{temp_response\}" ]; then\
    echo "Error: Empty response received from Ollama API."\
    jq --arg error "Empty response" '.error = $error' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
    if [ $? -eq 0 ]; then\
      mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
    fi\
    rm "$\{temp_response\}"\
    exit 1\
  fi\
  \
  if grep -q "error" "$\{temp_response\}"; then\
    echo "Error in Ollama response:"\
    cat "$\{temp_response\}"\
    \
    # Extract error message for metrics log\
    local error_msg=$(jq -r '.error // "Unknown error"' "$\{temp_response\}" 2>/dev/null)\
    if [ -z "$error_msg" ] || [ "$error_msg" = "null" ]; then\
      error_msg=$(cat "$\{temp_response\}")\
    fi\
    \
    jq --arg error "$error_msg" '.error = $error' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
    \
    # Check if jq succeeded\
    if [ $? -eq 0 ]; then\
      mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
    else\
      echo "Warning: Failed to update metrics log with error information"\
    fi\
    \
    # For timeout errors, recommend a different model\
    if grep -q "timeout" "$\{temp_response\}"; then\
      echo ""\
      echo "Recommendation: The model '$\{model\}' appears to be too slow for this request."\
      echo "Try using a faster model or a smaller input file."\
      echo "Available faster models:"\
      \
      # List up to 3 faster models if available\
      for ((i=1; i<=$\{#models[@]\} && i<=3; i++)); do\
        local model_name="$\{models[$i]\}"\
        local complexity=$\{MODEL_COMPLEXITY[$model_name]\}\
        if [ -n "$complexity" ] && (( $(echo "$complexity < $\{MODEL_COMPLEXITY[$model]\}" | bc -l 2>/dev/null) )); then\
          echo "- $\{model_name\}"\
        fi\
      done\
    fi\
    \
    rm "$\{temp_response\}"\
    exit 1\
  fi\
  \
  # Extract metrics from response if available\
  local total_duration=$(jq -r '.total_duration // 0' "$\{temp_response\}")\
  local eval_count=$(jq -r '.eval_count // 0' "$\{temp_response\}")\
  local prompt_eval_count=$(jq -r '.prompt_eval_count // 0' "$\{temp_response\}")\
  local eval_duration=$(jq -r '.eval_duration // 0' "$\{temp_response\}")\
  local prompt_eval_duration=$(jq -r '.prompt_eval_duration // 0' "$\{temp_response\}")\
  \
  # Convert to seconds if in nanoseconds\
  if [ "$\{total_duration\}" -gt 1000000000 ]; then\
    total_duration=$(printf "%.2f" $(echo "scale=2; $\{total_duration\}/1000000000" | bc))\
  fi\
  \
  # Add Ollama-reported metrics to log\
  jq --arg total "$\{total_duration\}" \\\
     --arg eval "$\{eval_count\}" \\\
     --arg prompt_eval "$\{prompt_eval_count\}" \\\
     --arg eval_dur "$\{eval_duration\}" \\\
     --arg prompt_dur "$\{prompt_eval_duration\}" \\\
     '.ollama_metrics = \{"total_duration": $total, "eval_count": $eval, "prompt_eval_count": $prompt_eval, "eval_duration": $eval_dur, "prompt_eval_duration": $prompt_dur\}' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
     \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with Ollama metrics"\
  fi\
  \
  # Log the raw response for debugging\
  if [ -s "$\{temp_response\}" ]; then\
    cp "$\{temp_response\}" "$\{BENCHMARK_DIR\}/raw_response_$\{SESSION_ID\}.json"\
  fi\
  \
  # Extract response content - try different methods for compatibility\
  if jq -e '.message.content' "$\{temp_response\}" > /dev/null 2>&1; then\
    RESPONSE=$(jq -r '.message.content' "$\{temp_response\}")\
  else\
    # Fallback method using grep and sed\
    RESPONSE=$(grep -o '"content":"[^"]*"' "$\{temp_response\}" | sed 's/"content":"//;s/"//')\
    \
    # If that fails, try perl\
    if [ -z "$\{RESPONSE\}" ]; then\
      RESPONSE=$(perl -0777 -ne 'print $1 if /"content":"(.*?)"/s' "$\{temp_response\}")\
    fi\
  fi\
  \
  # If still empty, try one more approach\
  if [ -z "$\{RESPONSE\}" ]; then\
    RESPONSE=$(cat "$\{temp_response\}" | python3 -c "\
import json, sys\
try:\
    data = json.load(sys.stdin)\
    print(data.get('message', \{\}).get('content', ''))\
except Exception as e:\
    print(f'Error parsing JSON: \{e\}', file=sys.stderr)\
    sys.exit(1)\
" 2>/dev/null)\
  fi\
  \
  # Last resort - try direct extraction for non-standard formats\
  if [ -z "$\{RESPONSE\}" ]; then\
    # Check for any JSON with a content field\
    RESPONSE=$(cat "$\{temp_response\}" | python3 -c "\
import json, sys, re\
try:\
    # First try to parse as regular JSON\
    data = json.load(sys.stdin)\
    # Try to find any field named 'content' recursively\
    def find_content(obj):\
        if isinstance(obj, dict):\
            if 'content' in obj:\
                return obj['content']\
            if 'message' in obj and isinstance(obj['message'], dict) and 'content' in obj['message']:\
                return obj['message']['content']\
            for k, v in obj.items():\
                content = find_content(v)\
                if content:\
                    return content\
        elif isinstance(obj, list):\
            for item in obj:\
                content = find_content(item)\
                if content:\
                    return content\
        return None\
    \
    content = find_content(data)\
    if content:\
        print(content)\
    else:\
        # Try regex as last resort\
        raw = json.dumps(data)\
        match = re.search(r'\\"content\\":\\"(.*?)\\"', raw)\
        if match:\
            print(match.group(1))\
except Exception as e:\
    # Try regex directly on the raw input\
    try:\
        raw = sys.stdin.read()\
        match = re.search(r'\\"content\\":\\"(.*?)\\"', raw)\
        if match:\
            print(match.group(1).replace('\\\\\\\\n', '\\\\n').replace('\\\\n', '\\n').replace('\\\\"', '"').replace('\\\\\\\\', '\\\\'))\
    except Exception as e2:\
        print(f'Error parsing content: \{e2\}', file=sys.stderr)\
        sys.exit(1)\
" 2>/dev/null)\
  fi\
  \
  # Check if response is empty\
  if [ -z "$\{RESPONSE\}" ]; then\
    echo -e "$\{RED\}Error: Empty or unparseable response from Ollama.$\{RESET\}"\
    echo -e "$\{YELLOW\}Raw response (first 500 characters):$\{RESET\}"\
    head -c 500 "$\{temp_response\}" | cat\
    echo "..."\
    \
    # Try an alternative approach to extract content\
    RESPONSE=$(cat "$\{temp_response\}" | python3 -c "\
import json, sys, re\
try:\
    data = json.load(sys.stdin)\
    if 'message' in data and 'content' in data['message']:\
        print(data['message']['content'])\
    else:\
        # Try extracting with regex as a fallback\
        content = sys.stdin.read()\
        match = re.search(r'\\"content\\":\\"(.*?)\\"', content, re.DOTALL)\
        if match:\
            print(match.group(1))\
except Exception as e:\
    print(f'Error extracting content: \{e\}', file=sys.stderr)\
" 2>/dev/null)\
\
    # If we got a response with the fallback method, use it\
    if [ -n "$\{RESPONSE\}" ]; then\
        echo -e "$\{GREEN\}Successfully extracted content using alternative method$\{RESET\}"\
    else\
        # Save the problematic response for debugging\
        cp "$\{temp_response\}" "$\{BENCHMARK_DIR\}/error_response_$\{SESSION_ID\}.json"\
        echo "Full response saved to: $\{BENCHMARK_DIR\}/error_response_$\{SESSION_ID\}.json"\
        \
        jq --arg error "Empty or unparseable response" --arg file "$\{BENCHMARK_DIR\}/error_response_$\{SESSION_ID\}.json" '.error = $error + " (saved to " + $file + ")"' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
    \
        # Check if jq succeeded\
        if [ $? -eq 0 ]; then\
          mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
        else\
          echo "Warning: Failed to update metrics log with empty response error"\
        fi\
    \
        # Suggest trying a different model\
        echo ""\
        echo -e "$\{YELLOW\}Recommendation: The model '$\{model\}' may be incompatible with this request format.$\{RESET\}"\
        echo -e "$\{YELLOW\}Try using a different model. Available alternatives:$\{RESET\}"\
        \
        # List up to 3 alternative models if available\
        local count=0\
        for ((i=1; i<=$\{#models[@]\} && count<3; i++)); do\
          local model_name="$\{models[$i]\}"\
          if [ "$model_name" != "$model" ]; then\
            echo -e "- $\{CYAN\}$\{model_name\}$\{RESET\}"\
            count=$((count + 1))\
          fi\
        done\
        \
        rm "$\{temp_response\}"\
        exit 1\
    fi\
  fi\
  \
  # Calculate response metrics\
  local response_char_count=$(echo "$\{RESPONSE\}" | wc -c | tr -d ' ')\
  local response_line_count=$(echo "$\{RESPONSE\}" | wc -l | tr -d ' ')\
  local response_word_count=$(echo "$\{RESPONSE\}" | wc -w | tr -d ' ')\
  \
  # Add response metrics to log\
  jq --arg chars "$\{response_char_count\}" \\\
     --arg lines "$\{response_line_count\}" \\\
     --arg words "$\{response_word_count\}" \\\
     '.response_metrics = \{"char_count": $chars, "line_count": $lines, "word_count": $words\}' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
     \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with response metrics"\
  fi\
  \
  # Save the full response for analysis\
  echo "$\{RESPONSE\}" > "$\{BENCHMARK_DIR\}/response_$\{SESSION_ID\}.md"\
  \
  # Clean up\
  rm "$\{temp_response\}"\
  \
  local parse_end_time=$(date +%s.%N)\
  local parse_duration=$(printf "%.2f" $(echo "$\{parse_end_time\} - $\{parse_start_time\}" | bc))\
  \
  log_benchmark "$\{script_basename\}" "$\{FILE_SIZE\}" "$\{LINE_COUNT\}" "$\{CHAR_COUNT\}" "$\{model\}" "response_parsing" "$\{parse_duration\}" "$\{response_word_count\}"\
  \
  # Create a new README.md file each time\
  local timestamp=$(date '+%Y-%m-%d %H:%M:%S')\
  local readme_filename="README_$\{script_basename\}_$\{model// /_\}.md"\
  README="$(pwd)/$\{readme_filename\}"\
  \
  echo -e "$\{BLUE\}Creating new README file: $\{GREEN\}$\{readme_filename\}$\{BLUE\}...$\{RESET\}"\
  \
  # Create a new README file\
  \{\
    echo "# $\{APP_NAME\} Documentation"\
    echo "Generated by $\{APP_NAME\} v$\{APP_VERSION\}"\
    echo "Author: $\{APP_AUTHOR\}"\
    echo "Date: $(date '+%Y-%m-%d')"\
    echo ""\
    echo "## $\{script_basename\} (Analyzed with $\{model\})"\
    echo "#### Analysis Date: $\{timestamp\}"\
    echo "#### Script Metrics: $\{FILE_SIZE\} bytes, $\{LINE_COUNT\} lines"\
    echo ""\
    echo "$\{RESPONSE\}"\
    echo ""\
    echo "## Additional Information"\
    echo ""\
    echo "### Script Analysis Metadata"\
    echo ""\
    echo "| Metadata | Value |"\
    echo "|----------|-------|"\
    echo "| Analysis Model | $\{model\} |"\
    echo "| Analysis Date | $\{timestamp\} |"\
    echo "| File Size | $\{FILE_SIZE\} bytes ($(printf "%.2f" $(echo "scale=2; $\{FILE_SIZE\}/1024" | bc)) KB) |"\
    echo "| Line Count | $\{LINE_COUNT\} |"\
    echo "| Character Count | $\{CHAR_COUNT\} |"\
    echo "| Analysis Time | $\{total_duration\}s |"\
    echo "| Model Response Time | $\{request_duration\}s |"\
    echo ""\
    echo "### Model Performance"\
    echo ""\
    echo "This README was automatically generated using $\{model\}, a large language model designed for code analysis and documentation. The model processed the script and produced this documentation in $\{total_duration\} seconds."\
    echo ""\
    echo "### License"\
    echo "This script is provided under the MIT License."\
    echo ""\
    echo "MIT License"\
    echo ""\
    echo "Copyright (c) $(date +%Y) $\{APP_AUTHOR\}"\
    echo ""\
    echo "Permission is hereby granted, free of charge, to any person obtaining a copy"\
    echo "of this software and associated documentation files (the \\"Software\\"), to deal"\
    echo "in the Software without restriction, including without limitation the rights"\
    echo "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell"\
    echo "copies of the Software, and to permit persons to whom the Software is"\
    echo "furnished to do so, subject to the following conditions:"\
    echo ""\
    echo "The above copyright notice and this permission notice shall be included in all"\
    echo "copies or substantial portions of the Software."\
    echo ""\
    echo "THE SOFTWARE IS PROVIDED \\"AS IS\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR"\
    echo "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,"\
    echo "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE"\
    echo "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER"\
    echo "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,"\
    echo "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE"\
    echo "SOFTWARE."\
  \} > "$\{README\}"\
  \
  echo -e "$\{GREEN\}$\{BOLD\}Created README for $\{script_basename\}$\{RESET\}"\
  \
  local end_time=$(date +%s.%N)\
  local total_duration=$(printf "%.2f" $(echo "$\{end_time\} - $\{start_time\}" | bc))\
  \
  # Log total analysis time\
  log_benchmark "$\{script_basename\}" "$\{FILE_SIZE\}" "$\{LINE_COUNT\}" "$\{CHAR_COUNT\}" "$\{model\}" "total_analysis" "$\{total_duration\}" "$\{response_word_count\}"\
  \
  # Update model performance database\
  update_model_performance "$\{model\}" "$\{FILE_SIZE\}" "$\{request_duration\}"\
  \
  # Update changelog\
  update_changelog "$\{script_basename\}" "$\{model\}" "$\{total_duration\}"\
  \
  # Generate benchmark summary\
  generate_benchmark_summary "$\{model\}" "$\{script_basename\}" "$\{total_duration\}" "$\{request_duration\}" "$\{parse_duration\}" "$\{FILE_SIZE\}" "$\{LINE_COUNT\}" "$\{CHAR_COUNT\}" "$\{response_word_count\}" "$\{estimated_seconds\}"\
  \
  # Show a random tip\
  show_tip\
  \
  # Add final timestamp to metrics log\
  jq --arg ts "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" '.completion_timestamp = $ts' "$\{METRICS_LOG\}" > "$\{METRICS_LOG\}.tmp" 2>/dev/null\
  \
  # Check if jq succeeded\
  if [ $? -eq 0 ]; then\
    mv "$\{METRICS_LOG\}.tmp" "$\{METRICS_LOG\}"\
  else\
    echo "Warning: Failed to update metrics log with completion timestamp"\
  fi\
  \
  return 0\
\}\
\
# =================== MAIN SCRIPT ===================\
\
# Start timer for overall execution\
SCRIPT_START_TIME=$(date +%s.%N)\
\
# Get system information\
get_system_info\
\
# Check for options\
SKIP_ESTIMATE=false\
\
# Process command-line arguments\
while [[ "$1" == --* ]]; do\
  case "$1" in\
    --help)\
      show_usage\
      ;;\
    --version)\
      show_version\
      ;;\
    --list-models)\
      get_models --list\
      ;;\
    --no-estimate)\
      SKIP_ESTIMATE=true\
      shift\
      ;;\
    --debug)\
      # Already handled at the top\
      shift\
      ;;\
    *)\
      echo "Unknown option: $1"\
      echo "Run '$0 --help' for usage information."\
      exit 1\
      ;;\
  esac\
done\
\
# Input handling\
if [ $# -lt 1 ]; then\
  echo "Error: No input file specified."\
  echo "Run '$0 --help' for usage information."\
  exit 1\
fi\
\
INPUT="$1"\
echo -e "$\{BLUE\}Input file:$\{RESET\} $\{YELLOW\}$\{INPUT\}$\{RESET\}"\
\
# Check dependencies\
check_dependencies || exit 1\
\
# Get available models\
get_models\
\
# Select model and generate README\
select_model "$@"\
validate_input_file "$\{INPUT\}"\
generate_readme "$\{INPUT\}" "$\{model\}" "$\{SKIP_ESTIMATE\}"\
\
# End timer for overall execution\
SCRIPT_END_TIME=$(date +%s.%N)\
TOTAL_EXECUTION_TIME=$(printf "%.2f" $(echo "$\{SCRIPT_END_TIME\} - $\{SCRIPT_START_TIME\}" | bc))\
\
log_benchmark "$\{INPUT\}" "$\{FILE_SIZE\}" "$\{LINE_COUNT\}" "$\{CHAR_COUNT\}" "$\{model\}" "script_execution" "$\{TOTAL_EXECUTION_TIME\}" "0"\
\
echo -e "$\{GREEN\}$\{BOLD\}Script completed in $\{TOTAL_EXECUTION_TIME\} seconds$\{RESET\}"\
echo -e "$\{BLUE\}Detailed metrics saved to:$\{RESET\}"\
echo -e "- README: $\{CYAN\}$\{README\}$\{RESET\}"\
echo -e "- CSV log: $\{CYAN\}$\{BENCHMARK_LOG\}$\{RESET\}"\
echo -e "- JSON metrics: $\{CYAN\}$\{METRICS_LOG\}$\{RESET\}"\
echo -e "- Response: $\{CYAN\}$\{BENCHMARK_DIR\}/response_$\{SESSION_ID\}.md$\{RESET\}"\
echo -e "- Changelog: $\{CYAN\}$\{CHANGELOG\}$\{RESET\}"\
}